import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

class ConsolidatedVisualizer:
    def __init__(self, model_manager):
        self.model_manager = model_manager
        self.model_colors = {
            "mobilenet_v2": "#1f77b4",  # blue
            "efficient_cnn": "#2ca02c", # green
            "resnet18": "#d62728"       # red
        }
        self.model_names = {
            "mobilenet_v2": "MobileNetV2",
            "efficient_cnn": "Efficient CNN", 
            "resnet18": "ResNet18"
        }
        self.dataset_names = {
            "mnist": "MNIST",
            "fashion": "Fashion-MNIST", 
            "cifar10": "CIFAR-10"
        }

    def plot_consolidated_training_history(self, datasets=["mnist", "fashion", "cifar10"]):
        """Plot training history comparison for all models on each dataset"""
        print("\n Generating consolidated training history...")
        
        n_datasets = len(datasets)
        fig, axes = plt.subplots(2, n_datasets, figsize=(6*n_datasets, 10))
        
        if n_datasets == 1:
            axes = axes.reshape(2, 1)
        
        for dataset_idx, dataset in enumerate(datasets):
            for model_type in ["mobilenet_v2", "efficient_cnn", "resnet18"]:
                history = self.model_manager.load_history(dataset, model_type)
                if history is None:
                    continue
                
                # Accuracy plot
                train_acc = history.get("accuracy", [])
                val_acc = history.get("val_accuracy", [])
                axes[0, dataset_idx].plot(
                    train_acc, 
                    label=f'{self.model_names[model_type]} Train', 
                    color=self.model_colors[model_type], 
                    linestyle='-', 
                    alpha=0.8,
                    linewidth=2
                )
                axes[0, dataset_idx].plot(
                    val_acc, 
                    label=f'{self.model_names[model_type]} Val', 
                    color=self.model_colors[model_type], 
                    linestyle='--', 
                    alpha=0.8,
                    linewidth=2
                )
                
                # Loss plot
                train_loss = history.get("loss", [])
                val_loss = history.get("val_loss", [])
                axes[1, dataset_idx].plot(
                    train_loss, 
                    label=f'{self.model_names[model_type]} Train', 
                    color=self.model_colors[model_type], 
                    linestyle='-', 
                    alpha=0.8,
                    linewidth=2
                )
                axes[1, dataset_idx].plot(
                    val_loss, 
                    label=f'{self.model_names[model_type]} Val', 
                    color=self.model_colors[model_type], 
                    linestyle='--', 
                    alpha=0.8,
                    linewidth=2
                )
            
            # Format accuracy subplot
            axes[0, dataset_idx].set_title(
                f'{self.dataset_names[dataset]} - Accuracy', 
                fontsize=14, 
                fontweight='bold'
            )
            axes[0, dataset_idx].set_xlabel('Epoch')
            axes[0, dataset_idx].set_ylabel('Accuracy')
            axes[0, dataset_idx].legend()
            axes[0, dataset_idx].grid(True, alpha=0.3)
            
            # Format loss subplot
            axes[1, dataset_idx].set_title(
                f'{self.dataset_names[dataset]} - Loss', 
                fontsize=14, 
                fontweight='bold'
            )
            axes[1, dataset_idx].set_xlabel('Epoch')
            axes[1, dataset_idx].set_ylabel('Loss')
            axes[1, dataset_idx].legend()
            axes[1, dataset_idx].grid(True, alpha=0.3)
        
        plt.suptitle(
            'Model Training History Comparison Across Datasets', 
            fontsize=16, 
            fontweight='bold',
            y=0.95
        )
        plt.tight_layout()
        try:
            import os
            if not os.path.exists('images'):
                os.makedirs('images')
            plt.savefig('images/Figure_1.png')
            print(" Saved training history comparison to images/Figure_1.png")
        except Exception as e:
            print(f" Failed to save plot: {e}")
        plt.show()

    def plot_performance_comparison(self, datasets=["mnist", "fashion", "cifar10"]):
        """Plot performance comparison across models and datasets"""
        print("\n Generating performance comparison...")
        
        from data_processor import DataProcessor
        dp = DataProcessor()
        results = []
        
        for dataset in datasets:
            (x_train, y_train), (x_test, y_test) = dp.load_dataset(dataset)
            x_train_norm, x_test_norm = dp.apply_normalization(x_train, x_test)
            
            for model_type in ["mobilenet_v2", "efficient_cnn", "resnet18"]:
                model = self.model_manager.load_model(dataset, model_type)
                if model is None:
                    continue
                
                y_probs = model.predict(x_test_norm, verbose=0)
                y_pred = np.argmax(y_probs, axis=1)
                
                accuracy = accuracy_score(y_test, y_pred)
                precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')
                
                results.append({
                    'Dataset': self.dataset_names[dataset],
                    'Model': self.model_names[model_type],
                    'Accuracy': accuracy,
                    'Precision': precision,
                    'Recall': recall,
                    'F1-Score': f1
                })
        
        # Convert to DataFrame for easier plotting
        df = pd.DataFrame(results)
        
        # Create subplots
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))
        
        # Accuracy comparison
        pivot_acc = df.pivot(index='Dataset', columns='Model', values='Accuracy')
        pivot_acc.plot(kind='bar', ax=ax1, color=[self.model_colors[mt] for mt in ["mobilenet_v2", "efficient_cnn", "resnet18"]])
        ax1.set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')
        ax1.set_ylabel('Accuracy')
        ax1.legend(title='Model')
        ax1.grid(True, alpha=0.3)
        
        # Precision comparison
        pivot_prec = df.pivot(index='Dataset', columns='Model', values='Precision')
        pivot_prec.plot(kind='bar', ax=ax2, color=[self.model_colors[mt] for mt in ["mobilenet_v2", "efficient_cnn", "resnet18"]])
        ax2.set_title('Macro Precision Comparison', fontsize=14, fontweight='bold')
        ax2.set_ylabel('Precision')
        ax2.legend(title='Model')
        ax2.grid(True, alpha=0.3)
        
        # Recall comparison
        pivot_rec = df.pivot(index='Dataset', columns='Model', values='Recall')
        pivot_rec.plot(kind='bar', ax=ax3, color=[self.model_colors[mt] for mt in ["mobilenet_v2", "efficient_cnn", "resnet18"]])
        ax3.set_title('Macro Recall Comparison', fontsize=14, fontweight='bold')
        ax3.set_ylabel('Recall')
        ax3.legend(title='Model')
        ax3.grid(True, alpha=0.3)
        
        # F1-Score comparison
        pivot_f1 = df.pivot(index='Dataset', columns='Model', values='F1-Score')
        pivot_f1.plot(kind='bar', ax=ax4, color=[self.model_colors[mt] for mt in ["mobilenet_v2", "efficient_cnn", "resnet18"]])
        ax4.set_title('Macro F1-Score Comparison', fontsize=14, fontweight='bold')
        ax4.set_ylabel('F1-Score')
        ax4.legend(title='Model')
        ax4.grid(True, alpha=0.3)
        
        plt.suptitle('Comprehensive Model Performance Comparison', fontsize=16, fontweight='bold')
        plt.tight_layout()
        try:
            import os
            if not os.path.exists('images'):
                os.makedirs('images')
            plt.savefig('images/testaccuracy.png')
            print(" Saved performance comparison to images/testaccuracy.png")
        except Exception as e:
            print(f" Failed to save plot: {e}")
        plt.show()
        
        return df

    def plot_model_complexity_analysis(self):
        """Plot model complexity vs performance analysis"""
        print("\n Generating model complexity analysis...")
        
        complexities = {
            'MobileNetV2': 2.1,
            'Efficient CNN': 3.8, 
            'ResNet18': 11.2
        }
        
        # Sample performance data (would be real data in practice)
        performance = {
            'MobileNetV2': 0.925,
            'Efficient CNN': 0.934,
            'ResNet18': 0.928
        }
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        for model in complexities:
            ax.scatter(
                complexities[model], 
                performance[model], 
                s=200, 
                alpha=0.7,
                label=model
            )
            ax.annotate(
                model, 
                (complexities[model], performance[model]),
                xytext=(5, 5), 
                textcoords='offset points',
                fontweight='bold'
            )
        
        ax.set_xlabel('Model Parameters (Millions)', fontsize=12, fontweight='bold')
        ax.set_ylabel('Accuracy on Fashion-MNIST', fontsize=12, fontweight='bold')
        ax.set_title('Model Complexity vs Performance Trade-off', fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()

    def generate_research_summary(self, datasets=["mnist", "fashion", "cifar10"]):
        """Generate comprehensive research summary"""
        print("\n" + "=" * 80)
        print(" COMPREHENSIVE RESEARCH SUMMARY")
        print("=" * 80)
        
        from data_processor import DataProcessor
        dp = DataProcessor()
        
        summary_data = []
        
        for dataset in datasets:
            dataset_info = dp.get_data_summary(dataset)
            print(f"\n Dataset: {dataset_info['name']}")
            print(f"   Shape: {dataset_info['shape']}")
            print(f"   Classes: {dataset_info['classes']}")
            
            for model_type in ["mobilenet_v2", "efficient_cnn", "resnet18"]:
                model = self.model_manager.load_model(dataset, model_type)
                if model is None:
                    print(f"   {self.model_names[model_type]}: Not trained")
                    continue
                
                (x_train, y_train), (x_test, y_test) = dp.load_dataset(dataset)
                x_train_norm, x_test_norm = dp.apply_normalization(x_train, x_test)
                
                y_probs = model.predict(x_test_norm, verbose=0)
                y_pred = np.argmax(y_probs, axis=1)
                accuracy = accuracy_score(y_test, y_pred)
                
                print(f"   {self.model_names[model_type]}: {accuracy:.4f}")
                
                summary_data.append({
                    'Dataset': dataset_info['name'],
                    'Model': self.model_names[model_type],
                    'Accuracy': accuracy,
                    'Parameters': model.count_params()
                })
        
        # Create summary DataFrame
        df_summary = pd.DataFrame(summary_data)
        
        print(f"\n Overall Findings:")
        best_model = df_summary.loc[df_summary['Accuracy'].idxmax()]
        print(f"   Best performing model: {best_model['Model']} on {best_model['Dataset']} ({best_model['Accuracy']:.4f})")
        
        most_efficient = df_summary.loc[df_summary.groupby('Dataset')['Accuracy'].idxmax()]
        print(f"\nüèÜ Best model per dataset:")
        for _, row in most_efficient.iterrows():
            print(f"   {row['Dataset']}: {row['Model']} ({row['Accuracy']:.4f})")
        
        return df_summary